{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b4a4771",
      "metadata": {
        "id": "7b4a4771"
      },
      "source": [
        "# Garbage Classification — Data Preprocessing & Inspection\n",
        "\n",
        "In this notebook, we will perform the full **data preprocessing and inspection** steps to make the dataset ready for modeling.  \n",
        "We aim to satisfy the following criteria:\n",
        "\n",
        "- Load & inspect the dataset (≈ 13.9k images)  \n",
        "- Confirm the 6 classes (`plastic`, `metal`, `glass`, `cardboard`, `paper`, `trash`)  \n",
        "- Verify class balance (≈ 2,300–2,500 images per class)  \n",
        "- Detect (and optionally remove or flag) duplicates  \n",
        "- Confirm image sizes and color channels (expected: 256×256, 3 channels RGB)  \n",
        "- Ensure labels align correctly with image files  \n",
        "- Split into train / validation / (test) sets, with stratification  \n",
        "- Normalize / standardize pixel values (record method)  \n",
        "- (Optional) Set up data augmentation  \n",
        "- Build a pipeline or loader to ensure a batch can go through a baseline CNN  \n",
        "\n",
        "We’ll break this into sections.\n",
        "\n",
        "---\n",
        "\n",
        "# Step 0: Download Kaggle Garbage Images Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IC-qV-5ER9Sw",
      "metadata": {
        "id": "IC-qV-5ER9Sw"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "# Upload your kaggle.json\n",
        "    # Only needs to be done once\n",
        "    # If you have not uploaded kaggle.json file here before,\n",
        "        # follow instructions below on acquiring kaggle.json\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHxCNBeeQHYV",
      "metadata": {
        "id": "HHxCNBeeQHYV"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "import os\n",
        "\n",
        "# Make sure the Kaggle API key is available\n",
        "if not os.path.exists(\"/root/.kaggle/kaggle.json\"):\n",
        "    print(\"UPLOAD KAGGLE.JSON FILE ABOVE\")\n",
        "    print(\"WATCH VIDEO I (AGUSTIN) SENT IN GROUP CHAT ON GETTING KAGGLE.JSON\")\n",
        "\n",
        "# Create data directory if not exists\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download + unzip only if not already present\n",
        "if not os.path.exists(\"data/Garbage_Dataset_Classification\"):\n",
        "    !kaggle datasets download -d zlatan599/garbage-dataset-classification -p data/\n",
        "    !unzip -q data/garbage-dataset-classification.zip -d data/\n",
        "    print(\"Dataset downloaded and extracted!\")\n",
        "else:\n",
        "    print(\"Dataset already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CvJXOxeDWUUs",
      "metadata": {
        "id": "CvJXOxeDWUUs"
      },
      "source": [
        "# Step 1: Setup & Imports (install if not already done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7036cb1",
      "metadata": {
        "collapsed": true,
        "id": "f7036cb1"
      },
      "outputs": [],
      "source": [
        "%pip install imagededup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75511c52",
      "metadata": {
        "id": "75511c52"
      },
      "outputs": [],
      "source": [
        "# Required libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import random\n",
        "import hashlib\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For dedup\n",
        "from imagededup.methods import PHash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c334de",
      "metadata": {
        "id": "c6c334de"
      },
      "source": [
        "# Step 2: Define dataset root & discover classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ecde58f",
      "metadata": {
        "id": "2ecde58f"
      },
      "outputs": [],
      "source": [
        "dataset_root = Path(\"data/Garbage_Dataset_Classification/images\")\n",
        "\n",
        "assert dataset_root.exists(), f\"Dataset root not found: {dataset_root}\"\n",
        "\n",
        "# List subdirectories as candidate classes\n",
        "classes = [d.name for d in dataset_root.iterdir() if d.is_dir()]\n",
        "classes = sorted(classes)\n",
        "print(\"Found classes:\", classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4172c5c2",
      "metadata": {
        "id": "4172c5c2"
      },
      "source": [
        "# Step 3: Confirm expected classes & label consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "172ae614",
      "metadata": {
        "id": "172ae614"
      },
      "outputs": [],
      "source": [
        "expected = {\"plastic\", \"metal\", \"glass\", \"cardboard\", \"paper\", \"trash\"}\n",
        "found = set(classes)\n",
        "print(\"Expected classes:\", expected)\n",
        "print(\"Found classes:\", found)\n",
        "\n",
        "if found == expected:\n",
        "    print(\"The classes match exactly the expected ones.\")\n",
        "else:\n",
        "    print(\"Class mismatch.\")\n",
        "    print(\"Missing:\", expected - found)\n",
        "    print(\"Extra:\", found - expected)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93712c4a",
      "metadata": {
        "id": "93712c4a"
      },
      "source": [
        "# Step 4: Count images per class & class balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd96254",
      "metadata": {
        "id": "efd96254"
      },
      "outputs": [],
      "source": [
        "# Supported Extensions\n",
        "SUPPORTED_EXTS = (\".jpg\", \".jpeg\", \".png\")\n",
        "\n",
        "class_counts = {}\n",
        "for cls in classes:\n",
        "    cls_dir = dataset_root / cls\n",
        "    imgs = []\n",
        "    for ext in SUPPORTED_EXTS:\n",
        "        imgs.extend(list(cls_dir.glob(f\"*{ext}\")))\n",
        "    # Also check for any unexpected extensions\n",
        "    other = list(cls_dir.glob(\"*\"))\n",
        "    others = [p for p in other if p.suffix.lower() not in SUPPORTED_EXTS]\n",
        "    if others:\n",
        "        print(f\"Warning: found {len(others)} files in {cls} with unexpected extension(s): {set(p.suffix for p in others)}\")\n",
        "    class_counts[cls] = len(imgs)\n",
        "\n",
        "print(\"Counts per class:\")\n",
        "for cls, cnt in class_counts.items():\n",
        "    print(f\"  {cls}: {cnt}\")\n",
        "counts = np.array(list(class_counts.values()))\n",
        "print(\"Total images:\", counts.sum())\n",
        "print(\"Min , Max , Mean:\", counts.min(), \",\", counts.max(), \",\", counts.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "542c58bd",
      "metadata": {
        "id": "542c58bd"
      },
      "source": [
        "# Step 5: Duplicate Detection, Preview & Cleaned Copy (change path in code block)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60165a3",
      "metadata": {
        "id": "f60165a3"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from imagededup.methods import PHash\n",
        "\n",
        "# 1. Run imagededup PHash per class\n",
        "ph = PHash()\n",
        "dups_all = {}\n",
        "\n",
        "for cls in classes:\n",
        "    cls_dir = dataset_root / cls\n",
        "    print(f\"Encoding class: {cls}\")\n",
        "    encodings = ph.encode_images(image_dir=str(cls_dir), recursive=False)\n",
        "    dups = ph.find_duplicates(encoding_map=encodings, max_distance_threshold=3)\n",
        "    dups_all[cls] = dups\n",
        "    n_dup_keys = len([k for k, v in dups.items() if v])\n",
        "    print(f\"  {n_dup_keys} keys have duplicates in {cls}\")\n",
        "\n",
        "# 2. Collect duplicate pairs across all classes\n",
        "def collect_duplicate_pairs(dups_dict, cls):\n",
        "    pairs = []\n",
        "    for fname, dup_list in dups_dict.items():\n",
        "        for dup in dup_list:\n",
        "            pairs.append((cls, fname, dup))\n",
        "    return pairs\n",
        "\n",
        "all_pairs = []\n",
        "for cls, dups in dups_all.items():\n",
        "    all_pairs.extend(collect_duplicate_pairs(dups, cls))\n",
        "\n",
        "print(f\"\\nTotal duplicate pairs found: {len(all_pairs)}\")\n",
        "\n",
        "# 3. Preview first 5 duplicate pairs\n",
        "def preview_duplicate_pairs(pairs, n=5):\n",
        "    for idx, (cls, f1, f2) in enumerate(pairs[:n]):\n",
        "        path1 = dataset_root / cls / f1\n",
        "        path2 = dataset_root / cls / f2\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
        "        try:\n",
        "            img1 = Image.open(path1)\n",
        "            img2 = Image.open(path2)\n",
        "\n",
        "            axes[0].imshow(img1)\n",
        "            axes[0].set_title(f\"{f1}\", fontsize=8)\n",
        "            axes[0].axis(\"off\")\n",
        "\n",
        "            axes[1].imshow(img2)\n",
        "            axes[1].set_title(f\"{f2}\", fontsize=8)\n",
        "            axes[1].axis(\"off\")\n",
        "\n",
        "            plt.suptitle(f\"Class: {cls} — Duplicate Pair {idx}\")\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {f1}, {f2}:\", e)\n",
        "\n",
        "preview_duplicate_pairs(all_pairs, n=5)\n",
        "\n",
        "# 4. Build cleaned dataset copy\n",
        "cleaned_root = Path(\"data/Garbage_Dataset_Classification/images_cleaned\")\n",
        "cleaned_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Mark duplicates to remove (always second file in pair)\n",
        "to_remove = set([p[2] for p in all_pairs])\n",
        "print(\"Total duplicate files to remove:\", len(to_remove))\n",
        "\n",
        "# Before counts\n",
        "print(\"\\nClass counts BEFORE cleaning:\")\n",
        "for cls in classes:\n",
        "    total = sum(len(list((dataset_root/cls).glob(f\"*{ext}\"))) for ext in SUPPORTED_EXTS)\n",
        "    print(f\"  {cls}: {total}\")\n",
        "\n",
        "# Copy files, skipping duplicates\n",
        "for cls in classes:\n",
        "    src_dir = dataset_root / cls\n",
        "    dst_dir = cleaned_root / cls\n",
        "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for ext in SUPPORTED_EXTS:\n",
        "        for file in src_dir.glob(f\"*{ext}\"):\n",
        "            if file.name not in to_remove:\n",
        "                shutil.copy(file, dst_dir / file.name)\n",
        "\n",
        "# After counts\n",
        "print(\"\\nClass counts AFTER cleaning:\")\n",
        "for cls in classes:\n",
        "    total = sum(len(list((cleaned_root/cls).glob(f\"*{ext}\"))) for ext in SUPPORTED_EXTS)\n",
        "    print(f\"  {cls}: {total}\")\n",
        "\n",
        "print(\"\\n Cleaned dataset created at:\", cleaned_root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2578750",
      "metadata": {
        "id": "d2578750"
      },
      "source": [
        "# Step 6: Confirm Image Sizes and Color Channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a6d2972",
      "metadata": {
        "id": "3a6d2972"
      },
      "outputs": [],
      "source": [
        "shape_counter = Counter()\n",
        "channel_counter = Counter()\n",
        "bad_images = []\n",
        "\n",
        "for cls in classes:\n",
        "    cls_dir = cleaned_root / cls\n",
        "    for ext in SUPPORTED_EXTS:\n",
        "        for p in cls_dir.glob(f\"*{ext}\"):\n",
        "            try:\n",
        "                with Image.open(p) as img:\n",
        "                    arr = np.array(img)\n",
        "                shape_counter[arr.shape] += 1\n",
        "                if arr.ndim == 3:\n",
        "                    channel_counter[arr.shape[2]] += 1\n",
        "                else:\n",
        "                    channel_counter[1] += 1\n",
        "            except Exception as e:\n",
        "                bad_images.append((p, str(e)))\n",
        "\n",
        "print(\"Image shape distribution (H, W, [C]):\")\n",
        "for shp, cnt in shape_counter.items():\n",
        "    print(f\"  {shp}: {cnt} images\")\n",
        "\n",
        "print(\"\\nChannel counts:\")\n",
        "for c, cnt in channel_counter.items():\n",
        "    print(f\"  {c} channels: {cnt} images\")\n",
        "\n",
        "print(\"\\nNumber of images that failed to load:\", len(bad_images))\n",
        "if bad_images:\n",
        "    print(\"Sample failed images:\", bad_images[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f605625b",
      "metadata": {
        "id": "f605625b"
      },
      "source": [
        "# Step 7: Verify Labels Align with Images (using metadata.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43777ffe",
      "metadata": {
        "id": "43777ffe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Verify every image in cleaned_root is inside the expected class folder\n",
        "problems = []\n",
        "for cls in classes:\n",
        "    cls_dir = cleaned_root / cls\n",
        "    for ext in SUPPORTED_EXTS:\n",
        "        for p in cls_dir.glob(f\"*{ext}\"):\n",
        "            if p.parent.name != cls:\n",
        "                problems.append((p, p.parent.name, cls))\n",
        "\n",
        "if problems:\n",
        "    print(f\"Found {len(problems)} images in the wrong folder:\")\n",
        "    print(problems[:10])\n",
        "else:\n",
        "    print(\"All images are in the correct class folders.\")\n",
        "\n",
        "# 2. Cross-check with metadata.csv located at the dataset's parent folder\n",
        "metadata_path = dataset_root.parent / \"metadata.csv\"\n",
        "\n",
        "if metadata_path.exists():\n",
        "    meta = pd.read_csv(metadata_path)\n",
        "    meta_map = dict(zip(meta.filename, meta.label))\n",
        "\n",
        "    mismatches = []\n",
        "    for cls in classes:\n",
        "        cls_dir = cleaned_root / cls\n",
        "        for ext in SUPPORTED_EXTS:\n",
        "            for p in cls_dir.glob(f\"*{ext}\"):\n",
        "                fname = p.name\n",
        "                if fname in meta_map and meta_map[fname] != cls:\n",
        "                    mismatches.append((fname, cls, meta_map[fname]))\n",
        "\n",
        "    if mismatches:\n",
        "        print(f\"Found {len(mismatches)} mismatches with metadata.csv:\")\n",
        "        print(mismatches[:10])\n",
        "    else:\n",
        "        print(\"Folder labels match metadata.csv for all files checked.\")\n",
        "else:\n",
        "    print(\"metadata.csv not found at:\", metadata_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6340922d",
      "metadata": {
        "id": "6340922d"
      },
      "source": [
        "# Step 8: Stratified Split using StratifiedShuffleSplit (80/10/10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a73da49d",
      "metadata": {
        "collapsed": true,
        "id": "a73da49d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from collections import Counter\n",
        "\n",
        "# Rebuild data list from cleaned dataset\n",
        "data = []\n",
        "for cls in classes:\n",
        "    cls_dir = cleaned_root / cls\n",
        "    for ext in SUPPORTED_EXTS:\n",
        "        for p in cls_dir.glob(f\"*{ext}\"):\n",
        "            data.append((p, cls))\n",
        "\n",
        "print(\"Total cleaned samples:\", len(data))\n",
        "\n",
        "paths = [p for p, lbl in data]\n",
        "labels = [lbl for p, lbl in data]\n",
        "\n",
        "# Choose your split ratios\n",
        "test_ratio = 0.10\n",
        "val_ratio = 0.10\n",
        "train_ratio = 1.0 - (test_ratio + val_ratio)\n",
        "assert train_ratio > 0, \"Make sure ratios sum to less than 1\"\n",
        "\n",
        "# 1. Split off test set\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=42)\n",
        "for train_valid_idx, test_idx in sss.split(paths, labels):\n",
        "    pass\n",
        "\n",
        "train_valid_paths = [paths[i] for i in train_valid_idx]\n",
        "train_valid_labels = [labels[i] for i in train_valid_idx]\n",
        "test_paths = [paths[i] for i in test_idx]\n",
        "test_labels = [labels[i] for i in test_idx]\n",
        "\n",
        "# 2. Split train_valid into train + validation\n",
        "rel_val = val_ratio / (train_ratio + val_ratio)\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=rel_val, random_state=42)\n",
        "for train_idx2, val_idx in sss2.split(train_valid_paths, train_valid_labels):\n",
        "    pass\n",
        "\n",
        "train_paths = [train_valid_paths[i] for i in train_idx2]\n",
        "train_labels = [train_valid_labels[i] for i in train_idx2]\n",
        "val_paths = [train_valid_paths[i] for i in val_idx]\n",
        "val_labels = [train_valid_labels[i] for i in val_idx]\n",
        "\n",
        "# Build final splits\n",
        "train_set = list(zip(train_paths, train_labels))\n",
        "valid_set = list(zip(val_paths, val_labels))\n",
        "test_set = list(zip(test_paths, test_labels))\n",
        "\n",
        "# Print sizes\n",
        "print(\"Total:\", len(data))\n",
        "print(\"Train:\", len(train_set), \"Validation:\", len(valid_set), \"Test:\", len(test_set))\n",
        "print()\n",
        "\n",
        "def print_dist(split, name):\n",
        "    c = Counter(lbl for _, lbl in split)\n",
        "    print(f\"{name} class counts:\")\n",
        "    for cls in classes:\n",
        "        print(f\"  {cls}: {c.get(cls, 0)}\")\n",
        "    print()\n",
        "\n",
        "print_dist(train_set, \"Train\")\n",
        "print_dist(valid_set, \"Validation\")\n",
        "print_dist(test_set, \"Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yf-ejdGz1O3h",
      "metadata": {
        "id": "Yf-ejdGz1O3h"
      },
      "source": [
        "# Step 9: Build PyTorch dataset & transforms (normalize + augment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aq_rUudx2X8N",
      "metadata": {
        "id": "aq_rUudx2X8N"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# Map class names to integer labels (sorted to be consistent)\n",
        "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
        "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "\n",
        "# Transforms\n",
        "# Train: light augmentations + normalize (ImageNet mean/std)\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.9, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "# Val/Test: center crop + normalize (no augmentation)\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "class GarbageDataset(Dataset):\n",
        "    def __init__(self, items, transform=None):\n",
        "        \"\"\"\n",
        "        items: list of (Path, class_name)\n",
        "        \"\"\"\n",
        "        self.items = items\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, cls_name = self.items[idx]\n",
        "        label = class_to_idx[cls_name]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label, str(path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v3VnJ7-z2md7",
      "metadata": {
        "id": "v3VnJ7-z2md7"
      },
      "source": [
        "# Step 10: Address class imbalance after cleaning (class weights or weighted sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v6BtLXyQ2vSv",
      "metadata": {
        "id": "v6BtLXyQ2vSv"
      },
      "outputs": [],
      "source": [
        "# Compute class counts from the TRAIN split\n",
        "train_class_counts = Counter(lbl for _, lbl in train_set)\n",
        "print(\"Train class counts:\", train_class_counts)\n",
        "\n",
        "# Option A: Class weights for CrossEntropyLoss: weight the loss function so false prediction on classes with fewer sample are harsher\n",
        "num_classes = len(classes)\n",
        "counts = np.array([train_class_counts[c] for c in classes], dtype=np.float32)\n",
        "class_weights = counts.sum() / (num_classes * counts)  # inverse-frequency-ish\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "print(\"Class weights (A):\", class_weights)\n",
        "\n",
        "# Option B: WeightedRandomSampler: classes with fewer sample are more likely to be chosen thus balance out class distribution\n",
        "label_to_idx = class_to_idx\n",
        "per_class_weight = {cls: (counts.sum() / (num_classes * cnt))\n",
        "                    for cls, cnt in train_class_counts.items()}\n",
        "sample_weights = [per_class_weight[lbl] for _, lbl in train_set]\n",
        "sampler = WeightedRandomSampler(weights=torch.DoubleTensor(sample_weights),\n",
        "                                num_samples=len(sample_weights),\n",
        "                                replacement=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rmGguqQK22WA",
      "metadata": {
        "id": "rmGguqQK22WA"
      },
      "source": [
        "# Step 11: DataLoaders (with augmentation on train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0pVDbdT-27KZ",
      "metadata": {
        "id": "0pVDbdT-27KZ"
      },
      "outputs": [],
      "source": [
        "# Datasets\n",
        "train_ds = GarbageDataset(train_set, transform=train_tfms)\n",
        "val_ds   = GarbageDataset(valid_set, transform=eval_tfms)\n",
        "test_ds  = GarbageDataset(test_set,  transform=eval_tfms)\n",
        "\n",
        "# DataLoaders: manage sampler, batch size, etc.\n",
        "BATCH_SIZE = 32\n",
        "num_workers = 2 if \"COLAB_GPU\" in os.environ or \"COLAB_TPU_ADDR\" in os.environ else 0\n",
        "\n",
        "use_weighted_sampler = True  # set False if we want to do option A\n",
        "\n",
        "if use_weighted_sampler:\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
        "                              sampler=sampler, num_workers=num_workers, pin_memory=True)\n",
        "else:\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
        "                         shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(\"Batches -> train:\", len(train_loader), \"val:\", len(val_loader), \"test:\", len(test_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J_eoHoWq3BC-",
      "metadata": {
        "id": "J_eoHoWq3BC-"
      },
      "source": [
        "# Step 12: Visualize one preprocessed + augmented batch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JSubVz-e3IsW",
      "metadata": {
        "id": "JSubVz-e3IsW"
      },
      "outputs": [],
      "source": [
        "# Helper to denormalize ImageNet-normalized tensors for display\n",
        "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
        "IMAGENET_STD  = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "def denormalize(img_tensor):\n",
        "    img = img_tensor.detach().cpu().numpy().transpose(1,2,0)\n",
        "    img = (img * IMAGENET_STD) + IMAGENET_MEAN\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "# Get one batch\n",
        "imgs, labels, paths = next(iter(train_loader))\n",
        "\n",
        "# Plot first 8\n",
        "n_show = min(8, imgs.size(0))\n",
        "plt.figure(figsize=(14, 6))\n",
        "for i in range(n_show):\n",
        "    plt.subplot(2, 4, i+1)\n",
        "    plt.imshow(denormalize(imgs[i]))\n",
        "    plt.title(idx_to_class[int(labels[i])])\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"Sample TRAIN batch (augmented + normalized)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Unmjiykn3Uj1",
      "metadata": {
        "id": "Unmjiykn3Uj1"
      },
      "source": [
        "# Step 13: Document split shapes (train/val/test per class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34oe1uIR3WrH",
      "metadata": {
        "id": "34oe1uIR3WrH"
      },
      "outputs": [],
      "source": [
        "def class_dist(items, title):\n",
        "    c = Counter(lbl for _, lbl in items)\n",
        "    print(title)\n",
        "    for cls in classes:\n",
        "        print(f\"  {cls:9s}: {c.get(cls,0)}\")\n",
        "    print(\"  TOTAL    :\", sum(c.values()))\n",
        "    print()\n",
        "\n",
        "class_dist(train_set, \"TRAIN distribution\")\n",
        "class_dist(valid_set, \"VALID distribution\")\n",
        "class_dist(test_set,  \"TEST  distribution\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5sSv1pA33onY",
      "metadata": {
        "id": "5sSv1pA33onY"
      },
      "source": [
        "# Step 14: Pipeline readiness: push one batch through ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "THYRN9T_3rR3",
      "metadata": {
        "id": "THYRN9T_3rR3"
      },
      "outputs": [],
      "source": [
        "# Load a baseline CNN and run a single forward pass to confirm pipeline is ready\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# starting from scratch\n",
        "# later we can use pretrained weights for TL\n",
        "model = models.resnet18(weights=None)\n",
        "# Adapt the final layer to 6 classes\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(classes))\n",
        "model = model.to(device)\n",
        "\n",
        "# One forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    xb, yb, _ = next(iter(train_loader))\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "    logits = model(xb)\n",
        "    print(\"Forward OK -> logits shape:\", logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 15: MobileNetV2 Architecture"
      ],
      "metadata": {
        "id": "KKpTCw8yxz_3"
      },
      "id": "KKpTCw8yxz_3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3b66e91",
      "metadata": {
        "id": "c3b66e91"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "# Depthwise Separable Convolution (used inside rInverted Residual)\n",
        "class Conv_BN_ReLU(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, groups=1):\n",
        "\n",
        "        padding = (kernel_size - 1) // 2 # to preserve original size if stride = 1\n",
        "\n",
        "        super().__init__(\n",
        "            # If group = 1, it's standard convolution (1 filter for entire depth)\n",
        "            # if groups = in_channels it's depthwise convolution (1 filter per layer)\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU6(inplace=True) # ReLU that clips value to be between [0, 6]\n",
        "        )\n",
        "\n",
        "# Inverted Residual Block\n",
        "class Inverted_Residual(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
        "        super().__init__()\n",
        "        # expand ratio control how much we expand channels before depthwise conv.\n",
        "        '''more channels/hidden_dim means more depth/layers or feature maps produced during 3x3 depthwise conv. means richer representation for learning'''\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "\n",
        "        '''a boolean that determine if residual short cut is applied at the end'''\n",
        "        self.use_res_connect = (stride == 1 and in_channels == out_channels)\n",
        "\n",
        "        layers = []\n",
        "        if expand_ratio != 1:\n",
        "            # carry out the expansion from in_channels to hidden_dim size if expand ratio is specified\n",
        "            layers.append(Conv_BN_ReLU(in_channels, hidden_dim, kernel_size=1))\n",
        "            '''Narrow -> Wide (recall Invert Residual Block is: Narrow -> Wide -> Narrow)'''\n",
        "\n",
        "        # applies Depthwise conv, 3x3\n",
        "        '''since groups=in_channels it means 1 filter per layer = Depthwise conv'''\n",
        "        layers.append(Conv_BN_ReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim))\n",
        "\n",
        "        # applies 1x1 conv: communication between depths\n",
        "        '''compress channels back to low-dimensional space (out_channels)'''\n",
        "        '''Wide -> Narrow'''\n",
        "        layers.append(nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "        # no ReLU because of linear bottleneck (prevents losing information in compressed representation)\n",
        "\n",
        "        '''the *layer is same as passing individual element of layer array, it's just easier this way'''\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        # warps the layers into a sequential block\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            # if dimension of in and out channel match at the start\n",
        "            # adds input to the output here at the end (residual short cut connection)\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    # width_mult to scale channels up/down\n",
        "    def __init__(self, num_classes=6, width_mult=1.0, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        inverted_residual_setting = [\n",
        "            # t (expand ratio), c (channels), n (repeats), s (stride)\n",
        "            [1, 16, 1, 1],\n",
        "            [6, 24, 2, 2],\n",
        "            [6, 32, 3, 2],\n",
        "            [6, 64, 4, 2],\n",
        "            [6, 96, 3, 1],\n",
        "            [6, 160, 3, 2],\n",
        "            [6, 320, 1, 1],\n",
        "        ]\n",
        "        '''\n",
        "            t: how much to widen channels inside the block (more channel=more layer/feature map produced during 3x3 depthwise convolution)\n",
        "            c: number of channels after each projection (output channels)\n",
        "            n: number of times to repeat the blocks\n",
        "            s: stride of first block, rest block all have stride = 1 (meaning down sample for first block)\n",
        "        '''\n",
        "\n",
        "        input_channel = int(32 * width_mult)\n",
        "        last_channel = int(1280*width_mult) if width_mult > 1.0 else 1280\n",
        "        # first conv layer outputs 32 channel scaled by width_mult\n",
        "        # final conv layer outputs 1280 channel scaled\n",
        "\n",
        "        '''add first standard 3x3 conv from RGB of 3 channel to 32 channel scaled'''\n",
        "        features = [Conv_BN_ReLU(in_channels=3, out_channels=input_channel, stride=2)]\n",
        "\n",
        "        '''loop through each stage Continuously adding layers: Expands -> depthwise -> projects\n",
        "           apply stride in first block = downsampling, then rest block keep stride=1'''\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = int(c * width_mult) # Expand\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1 # stride of 1 if not first block\n",
        "                features.append(Inverted_Residual(input_channel, output_channel, stride, expand_ratio=t)) # depthwise conv. and proj.\n",
        "                input_channel = output_channel # set the corresponding input channel for next layer\n",
        "\n",
        "        # add final 1x1 convolution\n",
        "        features.append(Conv_BN_ReLU(input_channel, last_channel, kernel_size=1))\n",
        "\n",
        "        # wrap all layer into a sequential block\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # define final linear classification layer added in forward function\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        '''initialize weight before forward pass'''\n",
        "        self._initialize_weights()\n",
        "\n",
        "    '''define forward pass:\n",
        "        1 extract features\n",
        "        2 apply global average pooling\n",
        "        3 fully connected layer for classification'''\n",
        "    def forward(self, x):\n",
        "        x = self.features(x) # continuous feature extraction\n",
        "        x = x.mean([2, 3])  # Add global average pooling layer\n",
        "        x = self.classifier(x) # linear layer for final classification\n",
        "        return x\n",
        "\n",
        "    # the \"_\" before function indicates it's an function for internal use\n",
        "    def _initialize_weights(self):\n",
        "        '''loop through all layers'''\n",
        "        for m in self.modules():\n",
        "            '''check if current layer is either 2D conv. or 2D batch norm. or a fully connected layer'''\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # value are normally distributed mean of 0 with variance scaled based on number of output unit (fan_out)\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "                if m.bias is not None: # initialize bias if exist in current layer\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                # weight initialize with mean 0 and deviation 0.01\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "039ca990",
      "metadata": {
        "id": "039ca990"
      },
      "source": [
        "# Step 16: Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7820150",
      "metadata": {
        "id": "c7820150"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "def model_train(model, train_loader, validation_loader, optimizer, criterion, scheduler, epochs, device):\n",
        "    # Tracking lists\n",
        "    epoch_list = []\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    train_precisions, val_precisions = [], []\n",
        "    train_recalls, val_recalls = [], []\n",
        "    train_f1s, val_f1s = [], []\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ===== TRAINING PHASE =====\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        for images, labels, paths in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(images)\n",
        "            loss = criterion(y_pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate training metrics\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_acc = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "\n",
        "        # Calculate precision, recall, F1\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            all_labels, all_preds, average='macro', zero_division=0\n",
        "        )\n",
        "\n",
        "        # ===== VALIDATION PHASE =====\n",
        "        val_metrics = model_evaluation(model, validation_loader, criterion, device)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
        "        print(f\"  Train -> Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | \"\n",
        "              f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
        "        print(f\"  Val   -> Loss: {val_metrics['loss']:.4f} | Acc: {val_metrics['accuracy']:.2f}% | \"\n",
        "              f\"Precision: {val_metrics['precision']:.4f} | Recall: {val_metrics['recall']:.4f} | \"\n",
        "              f\"F1: {val_metrics['f1']:.4f}\")\n",
        "\n",
        "        # Store metrics\n",
        "        epoch_list.append(epoch + 1)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        train_precisions.append(precision)\n",
        "        train_recalls.append(recall)\n",
        "        train_f1s.append(f1)\n",
        "\n",
        "        val_losses.append(val_metrics['loss'])\n",
        "        val_accuracies.append(val_metrics['accuracy'])\n",
        "        val_precisions.append(val_metrics['precision'])\n",
        "        val_recalls.append(val_metrics['recall'])\n",
        "        val_f1s.append(val_metrics['f1'])\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save best model based on validation F1 score\n",
        "        if val_metrics['f1'] > best_f1:\n",
        "            best_f1 = val_metrics['f1']\n",
        "            best_acc = val_metrics['accuracy']\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': best_f1,\n",
        "                'val_acc': best_acc,\n",
        "            }, 'best_model.pth')\n",
        "            print(f\"  ✓ New best model saved! (F1: {best_f1:.4f}, Acc: {best_acc:.2f}%)\")\n",
        "\n",
        "    return {\n",
        "        'epochs': epoch_list,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_accuracies': val_accuracies,\n",
        "        'train_precisions': train_precisions,\n",
        "        'val_precisions': val_precisions,\n",
        "        'train_recalls': train_recalls,\n",
        "        'val_recalls': val_recalls,\n",
        "        'train_f1s': train_f1s,\n",
        "        'val_f1s': val_f1s,\n",
        "        'best_f1': best_f1,\n",
        "        'best_acc': best_acc\n",
        "    }\n",
        "\n",
        "\n",
        "def model_evaluation(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, paths in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            y_pred = model(images)\n",
        "            loss = criterion(y_pred, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(y_pred, dim=1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    avg_loss = running_loss / len(data_loader.dataset)\n",
        "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "\n",
        "    # Calculate precision, recall, F1 (macro-averaged)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    per_class_precision, per_class_recall, per_class_f1, support = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'per_class_precision': per_class_precision,\n",
        "        'per_class_recall': per_class_recall,\n",
        "        'per_class_f1': per_class_f1,\n",
        "        'support': support,\n",
        "        'all_preds': all_preds,\n",
        "        'all_labels': all_labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0728c898",
      "metadata": {
        "id": "0728c898"
      },
      "source": [
        "# Step 17: Hyparameters & Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46581d36",
      "metadata": {
        "id": "46581d36"
      },
      "outputs": [],
      "source": [
        "# hyper parameter\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-4\n",
        "num_epochs = 1\n",
        "dropout_rate = 0.2\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Optional warning suppression\n",
        "    os.environ['PYTHONWARNINGS'] = 'ignore:semaphore_tracker:UserWarning'\n",
        "    # This ensures proper multiprocessing behavior\n",
        "    # If using multiprocessing DataLoader\n",
        "    import sys, torch.multiprocessing as mp\n",
        "    if sys.platform.startswith(\"linux\"):\n",
        "        mp.set_start_method('fork', force=True)   # works and fixed your case\n",
        "    else:\n",
        "        mp.set_start_method('spawn', force=True)  # safer on macOS/Windows\n",
        "\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = MobileNetV2(num_classes=6, width_mult=1.0, dropout_rate=dropout_rate).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Optimizer, loss, scheduler\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Train model\n",
        "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "    results = model_train(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, device)\n",
        "\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training Complete!\")\n",
        "    print(f\"Best Validation F1: {results['best_f1']:.4f}\")\n",
        "    print(f\"Best Validation Accuracy: {results['best_acc']:.2f}%\")\n",
        "    print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 18: Visualize Metrics"
      ],
      "metadata": {
        "id": "iLA2tLKp1it1"
      },
      "id": "iLA2tLKp1it1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Loss over epochs\n",
        "axes[0, 0].plot(results['epochs'], results['train_losses'], label='Train Loss', marker='o', linewidth=2)\n",
        "axes[0, 0].plot(results['epochs'], results['val_losses'], label='Val Loss', marker='s', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=11)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Accuracy over epochs\n",
        "axes[0, 1].plot(results['epochs'], results['train_accuracies'], label='Train Accuracy', marker='o', linewidth=2)\n",
        "axes[0, 1].plot(results['epochs'], results['val_accuracies'], label='Val Accuracy', marker='s', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=11)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Precision over epochs\n",
        "axes[1, 0].plot(results['epochs'], results['train_precisions'], label='Train Precision', marker='o', linewidth=2)\n",
        "axes[1, 0].plot(results['epochs'], results['val_precisions'], label='Val Precision', marker='s', linewidth=2)\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Precision', fontsize=12)\n",
        "axes[1, 0].set_title('Training and Validation Precision (Macro-Avg)', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=11)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: F1-Score over epochs\n",
        "axes[1, 1].plot(results['epochs'], results['train_f1s'], label='Train F1', marker='o', linewidth=2)\n",
        "axes[1, 1].plot(results['epochs'], results['val_f1s'], label='Val F1', marker='s', linewidth=2)\n",
        "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1, 1].set_ylabel('F1-Score', fontsize=12)\n",
        "axes[1, 1].set_title('Training and Validation F1-Score (Macro-Avg)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].legend(fontsize=11)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final results summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL TRAINING SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Best Validation Accuracy: {results['best_acc']:.2f}%\")\n",
        "print(f\"Best Validation F1-Score: {results['best_f1']:.4f}\")\n",
        "print(f\"\\nFinal Epoch Metrics:\")\n",
        "print(f\"  Train -> Loss: {results['train_losses'][-1]:.4f} | Acc: {results['train_accuracies'][-1]:.2f}% | F1: {results['train_f1s'][-1]:.4f}\")\n",
        "print(f\"  Val   -> Loss: {results['val_losses'][-1]:.4f} | Acc: {results['val_accuracies'][-1]:.2f}% | F1: {results['val_f1s'][-1]:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "hp3ALfKY1hsn"
      },
      "id": "hp3ALfKY1hsn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 19: Confusion Matrix Visualization"
      ],
      "metadata": {
        "id": "WC3zC1He2FRe"
      },
      "id": "WC3zC1He2FRe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate final model on validation set to get predictions for confusion matrix\n",
        "final_val_metrics = model_evaluation(model, val_loader, criterion, device)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(final_val_metrics['all_labels'], final_val_metrics['all_preds'])\n",
        "\n",
        "# Plot confusion matrix\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# Raw counts confusion matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=classes, yticklabels=classes,\n",
        "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
        "axes[0].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Normalized confusion matrix (percentages)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "            xticklabels=classes, yticklabels=classes,\n",
        "            ax=axes[1], cbar_kws={'label': 'Percentage'})\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Confusion Matrix (Normalized by True Class)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLASSIFICATION REPORT (Validation Set)\")\n",
        "print(\"=\"*70)\n",
        "print(classification_report(final_val_metrics['all_labels'],\n",
        "                          final_val_metrics['all_preds'],\n",
        "                          target_names=classes,\n",
        "                          digits=4))\n",
        "\n",
        "# Print per-class metrics in a table format\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PER-CLASS METRICS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
        "print(\"-\"*70)\n",
        "for i, cls in enumerate(classes):\n",
        "    print(f\"{cls:<12} {final_val_metrics['per_class_precision'][i]:<12.4f} \"\n",
        "          f\"{final_val_metrics['per_class_recall'][i]:<12.4f} \"\n",
        "          f\"{final_val_metrics['per_class_f1'][i]:<12.4f} \"\n",
        "          f\"{int(final_val_metrics['support'][i]):<10}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Macro Avg':<12} {final_val_metrics['precision']:<12.4f} \"\n",
        "      f\"{final_val_metrics['recall']:<12.4f} \"\n",
        "      f\"{final_val_metrics['f1']:<12.4f} \"\n",
        "      f\"{int(sum(final_val_metrics['support'])):<10}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "L9i1iwk92FmB"
      },
      "id": "L9i1iwk92FmB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}